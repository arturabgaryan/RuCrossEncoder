# RuCrossEncoder

The work aims to address the lack of high-performing, language-specific solutions for Russian and provides a comparative analysis with existing cross-encoders (e.g., MsMarco, FRIDA). By integrating DeBERTaâ€™s attention mechanisms with a classification layer tailored to Russian morphology and syntax, the model improves performance in semantic similarity, text classification, and question-answering systems

[Pre-trained weights](https://huggingface.co/ArturAbg/RuCrossEncoder) and [Generated data](https://huggingface.co/datasets/ArturAbg/GeneratedRURerank) can be found here